{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRHh09dz-XZO"
      },
      "source": [
        "# UNICC Chatbot\n",
        "\n",
        "In this notebook, we train the open-source Llama LLM on our dataset of UNECE documents, using a 4-bit quantized version from Unsloth to improve efficiency. To improve accuracy, we also introduced a RAG pipeline that identifies relevant parsed segments of the PDF database and passes them in as context to the Llama queries.\n",
        "\n",
        "## Contents\n",
        "\n",
        "1. Performing text splitting: parses PDF database of UNECE policy documents and session resolutions into 50-500 character \"chunks\" using font size, boldness, etc. to identify section headers. Stores these chunks along with document metadata to later feed into the RAG pipeline.\n",
        "\n",
        "  **1.1**. Uses llama-index (open source embedding library) to embed and store these chunks in a vector-based document index for later collection. Uses a traditional tf-idf scoring with cosine similarity for relevance evaluations.\n",
        "\n",
        "2. Llama 4-bit quantized: prepares the model itself, using unsloth to get a pre-trained 4-bit quantized version of Llama 3.1 8B Instruct. Re-loads the same PDFs from the text splitting phase but as entire documents to pass into the model for fine-tuning.\n",
        "\n",
        "    **2.1.** Uses LoRA for fine-tuning\n",
        "    **2.2.** Trains with SFTTrainer from hugging face\n",
        "\n",
        "3. Front end: basic chatbot website set up for demo purposes -- allows user to input questions, view responses, and interact with relevant documents based on submitted queries (collected from the chunk embeddings metadata). Uses ngrok to simulate a mini-server on Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LO4xxwxDNBgc"
      },
      "outputs": [],
      "source": [
        "# Text parsing packages\n",
        "!pip install pdfplumber\n",
        "!pip install fitz\n",
        "!pip install PyMuPDF\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEpbke6C1KHu"
      },
      "outputs": [],
      "source": [
        "# Imports for the RAG encodings\n",
        "!pip install llama-index\n",
        "!pip install llama-index-embeddings-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGfyIGKIcUnw"
      },
      "outputs": [],
      "source": [
        "# Imports for the model\n",
        "!pip install unsloth\n",
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1JDJ0M_e_81"
      },
      "outputs": [],
      "source": [
        "# Installs for frontend\n",
        "!pip install flask-ngrok pyngrok\n",
        "!ngrok authtoken 2ojOiPQ59Oi8KsIkY8xByZxp3xp_GJ1GTXSJfSimSNUKquke\n",
        "!pip install pdf2image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q15-rLSMmxp"
      },
      "source": [
        "# Performing text splitting\n",
        "\n",
        "Parsing PDF into chunks based on topic headers, which are then encoded to use in the RAG pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnJspE_qMsZM"
      },
      "outputs": [],
      "source": [
        "import fitz\n",
        "import os\n",
        "from google.colab import drive\n",
        "import glob\n",
        "import re\n",
        "\n",
        "def extract_chunks(pdf_path, title):\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    current_title = \"Introduction\"  # default title for the first chunk\n",
        "    current_title = title\n",
        "    tables = []\n",
        "    table_pattern = re.compile(r\"([A-Za-z0-9]+(\\s{2,}|,\\s?))+\")  # Pattern for detecting rows in tables\n",
        "\n",
        "    # bullet points pattern --> used to avoid tracking bullet itself as a section header\n",
        "    bullet_pattern = re.compile(r\"^[•●○‣▪■□–-]\\s\")\n",
        "\n",
        "    doc = fitz.open(pdf_path)\n",
        "    for page in doc:\n",
        "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
        "        for block in blocks:\n",
        "            if \"lines\" in block:\n",
        "                table_content = []\n",
        "                for line in block[\"lines\"]:\n",
        "\n",
        "                    #accounting for too big chunks\n",
        "                    if len(current_chunk) > 650:\n",
        "                        chunks.append({\"title\": title, \"chunk_title\": current_title, \"content\": current_chunk.strip()})\n",
        "                        current_chunk = \"\"\n",
        "                        chunk_title=\"\"\n",
        "\n",
        "                    line_text = \" \".join([span[\"text\"] for span in line[\"spans\"]]).strip()\n",
        "                    font_size = line[\"spans\"][0][\"size\"]\n",
        "                    font_name = line[\"spans\"][0][\"font\"]\n",
        "\n",
        "                    # heuristics for headers\n",
        "                    is_bold = \"Bold\" in font_name or \"SemiBold\" in font_name\n",
        "                    is_bullet = bullet_pattern.match(line_text)\n",
        "                    line_text = bullet_pattern.sub(\"\", line_text).strip()\n",
        "\n",
        "                    # detect numbers-only lines (eg page numbers)\n",
        "                    if line_text.isdigit():\n",
        "                        continue\n",
        "\n",
        "                    # heuristics for footnotes (choosing to ignore for now bc often not useful without context)\n",
        "                    if font_size < 10:\n",
        "                        continue\n",
        "\n",
        "                    # checking for tables\n",
        "                    is_table_line = table_pattern.match(line_text)\n",
        "                    if is_table_line:\n",
        "                        table_content.append(line_text)\n",
        "                        continue\n",
        "\n",
        "                    # checking for headers\n",
        "                    if (font_size >= 14 or is_bold) and not is_bullet:\n",
        "                        if current_chunk:\n",
        "                            chunks.append({\"title\": title, \"chunk_title\": current_title, \"content\": current_chunk.strip()})\n",
        "                            current_chunk = \"\"\n",
        "                        current_title = line_text\n",
        "                        current_chunk = line_text #adding the title to the chunk, just bc chunk_title is often not that specific and is messing us up\n",
        "                    else:\n",
        "                        current_chunk += \" \" + line_text\n",
        "\n",
        "                if table_content:\n",
        "                    tables.append({\"title\": title, \"chunk_title\": current_title, \"content\": \"\\n\".join(table_content)})\n",
        "                    table_content = []\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append({\"title\": title, \"chunk_title\": current_title,  \"content\": current_chunk.strip()})\n",
        "    if tables:\n",
        "        chunks.extend(tables)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "folder_path = '/content/drive/My Drive/UNICC_dataset/' #'/content/drive/My Drive/UNICC_db/'\n",
        "\n",
        "# Get all .pdf files in the folder\n",
        "#NOTE: you will have to make this file yourself in your own drive, it just contains all of the PDFS Jason gave us\n",
        "file_pattern = os.path.join(folder_path, '*.pdf')\n",
        "chunks = []\n",
        "for file_path in glob.glob(file_pattern):\n",
        "    filename = os.path.basename(file_path)\n",
        "    title = os.path.splitext(filename)[0]\n",
        "    print(\"Extracting passages from document:\", file_path)\n",
        "    chunks.extend(extract_chunks(file_path, title))\n",
        "\n",
        "#removing super short chunks, as these are not informative based on testing\n",
        "rem_chunks =  [chunk for chunk in chunks if len(chunk[\"content\"]) < 50 ]\n",
        "chunks =  [chunk for chunk in chunks if len(chunk[\"content\"]) >= 50 ]\n",
        "\n",
        "\"\"\"\n",
        "for chunk in chunks:\n",
        "    print(f\"Title: {chunk['title']}\")\n",
        "    print(f\"Chunk Title: {chunk['chunk_title']}\")\n",
        "    print(f\"Content length: {len(chunk['content'])}\")\n",
        "    print(f\"Content: {chunk['content'][:200]}...\")  # Print the first 200 characters of content\n",
        "    print(\"\\n\")\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlkdMX12a9G8"
      },
      "source": [
        "## Getting embeddings from chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtPxq_Aqa79A"
      },
      "outputs": [],
      "source": [
        "# using https://docs.llamaindex.ai/en/v0.9.48/examples/embeddings/huggingface.html\n",
        "# https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings/\n",
        "\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Document, Settings, VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.core.llms import MockLLM\n",
        "\n",
        "# we are only using llama_index for its embeddings and collecting relevant context, not its LLMs, so this is basically a placeholder\n",
        "llm = MockLLM()\n",
        "\n",
        "embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
        ")\n",
        "\n",
        "Settings.embed_model = embed_model\n",
        "\n",
        "# reformatting chunks found above into document form\n",
        "documents = [\n",
        "    Document(\n",
        "        text=chunk['content'],\n",
        "        metadata={\n",
        "            'document_title': chunk['title'],\n",
        "            'chunk_title': chunk['chunk_title']\n",
        "        }\n",
        "    ) for chunk in chunks\n",
        "]\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwsdAJrGD63E"
      },
      "outputs": [],
      "source": [
        "# testing relevant document retrieval\n",
        "query_engine = index.as_query_engine(embed_model=embed_model, llm=llm, similarity_top_k=10)\n",
        "\n",
        "response = query_engine.query(\"What is methane drainage?\")\n",
        "\n",
        "print(\"\\nRelevant documents found:\")\n",
        "for i, node in enumerate(response.source_nodes, start=1):\n",
        "    print(f\"\\nDocument {i}:\")\n",
        "    print(f\"Document Title: {node.node.metadata.get('document_title', 'No title available')}\")\n",
        "    print(f\"Chunk Title: {node.node.metadata.get('chunk_title', 'No chunk title available')}\")\n",
        "    print(f\"Content: {node.node.text}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh56kDpjzDWt"
      },
      "source": [
        "# Llama 4-bit quantized\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFHDgtZazDOI"
      },
      "outputs": [],
      "source": [
        "# FRom https://huggingface.co/unsloth/Meta-Llama-3.1-8B-bnb-4bit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2v_X2fA0Df5"
      },
      "source": [
        "* We support Llama, Mistral, Phi-3, Gemma, Yi, DeepSeek, Qwen, TinyLlama, Vicuna, Open Hermes etc\n",
        "* We support 16bit LoRA or 4bit QLoRA. Both 2x faster.\n",
        "* `max_seq_length` can be set to anything, since we do automatic RoPE Scaling via [kaiokendev's](https://kaiokendev.github.io/til) method.\n",
        "* [**NEW**] We make Gemma-2 9b / 27b **2x faster**! See our [Gemma-2 9b notebook](https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing)\n",
        "* [**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)\n",
        "* [**NEW**] We make Mistral NeMo 12B 2x faster and fit in under 12GB of VRAM! [Mistral NeMo notebook](https://colab.research.google.com/drive/17d3U-CAIwzmbDRqbZ9NnpHxCkmXB6LZ0?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# from unsloth docs at https://huggingface.co/unsloth\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\", #\"unsloth/Meta-Llama-3.1-70B-bnb-4bit\", # \"unsloth/Meta-Llama-3.1-8B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "e06767a0-ea55-4afb-99e1-9016e71e8c2d"
      },
      "outputs": [],
      "source": [
        "# adding LoRA adaptors -- use a very high lora_alpha to increase the impact of UNECE dataset over pre-training.\n",
        "# also use rank stabilized LoRA for slightly improved performance\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 64, # increased from 16 to increase the impact of our training dataset in comparison to the default training\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = True,  # rank stabilized LoRA\n",
        "    loftq_config = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "### Data Prep\n",
        "Parsing UNECE PDFS into text -- doesn't do any other processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpBKtkigt8wA"
      },
      "outputs": [],
      "source": [
        "import fitz\n",
        "import spacy\n",
        "import glob\n",
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "def extract_text(file_path):\n",
        "    doc = fitz.open(file_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    text = text.replace('\\n', '')\n",
        "    # make it all lower case\n",
        "    text = text.lower()\n",
        "    print(text)\n",
        "    return text\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# processing all PDFs\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "docs = []\n",
        "folder_path = '/content/drive/My Drive/UNICC_dataset' #UNICC_db for smaller sample\n",
        "file_pattern = os.path.join(folder_path, '*.pdf')\n",
        "for file_path in glob.glob(file_pattern):\n",
        "    print(f\"Processing file: {file_path}\")\n",
        "    docs.append(extract_text(file_path) + EOS_TOKEN)\n",
        "\n",
        "\n",
        "df = pd.DataFrame({\"text\": docs})\n",
        "df.to_csv('dataset.csv', index=False, escapechar='\\\\') #using escapechar bc our actual data contains comma\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)\n",
        "\n",
        "dataset = load_dataset('csv', data_files={'train': 'dataset.csv'}, split='train')\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# add labels by copying input_ids\n",
        "def add_labels(batch):\n",
        "    batch['labels'] = batch['input_ids'].copy()\n",
        "    return batch\n",
        "\n",
        "tokenized_datasets = tokenized_datasets.map(add_labels, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "SFT Docs (chosen based on unsloth docs): (https://huggingface.co/docs/trl/sft_trainer) -- train with max_steps for now to shorted training process and reduce compute on Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        #num_train_epochs = 1, # set this for 1 full training run.\n",
        "        max_steps = 10,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "# training!\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# setting api key from colab secrets\n",
        "os.environ[\"WANDB_API_KEY\"] = userdata.get('wandb-api-key')\n",
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jj9PYsMhYCsQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Example questions:  Why is it important to reduce gas emissions?\n",
        "# From what type of mine do most coal mine emissions come from?\n",
        "# When and where did the first occurance of methane drainage take place?\n",
        "\n",
        "conv_history = [] # list of question - response strings\n",
        "context = \"\"\n",
        "max_conv_len = 1024 # used to trim conv history if it's getting too long\n",
        "\n",
        "\"\"\" returns a formatted string of the conversation history to pass into a prompt \"\"\"\n",
        "def get_conv_hist():\n",
        "    global conv_history\n",
        "    formatted_conv = [(\"User: \" + conv if i % 2 == 0 else \"AI assistant: \" + conv) for i, conv in enumerate(conv_history)]\n",
        "    formatted_conv= \"\\n\".join(formatted_conv)\n",
        "    return formatted_conv\n",
        "\n",
        "\"\"\" adds text to the existing conv history, and reduces len if it exceeds max_conv_len \"\"\"\n",
        "def set_conv_hist(text):\n",
        "    global conv_history\n",
        "    conv_history.append(text)\n",
        "\n",
        "    # cutting off oldest parts of the conversation in pairs of 2 (question + answer)\n",
        "    while len(\"\\n\".join(conv_history)) > max_conv_len:\n",
        "      conv_history = conv_history[2:]\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Uses the query engine created during the \"Getting embeddings from chunks\" section\n",
        "to identify the most similar document chunks\n",
        "given a specific query.\n",
        "\"\"\"\n",
        "def get_context(question):\n",
        "    # adding context from the identified similar chunks\n",
        "    global context\n",
        "    cur_cont = \"\"\n",
        "    titles = []\n",
        "    query_engine = index.as_query_engine(embed_model=embed_model, llm=llm, similarity_top_k=10)\n",
        "    question_history = [conv if i%2 == 0 else \"\" for i, conv in enumerate(conv_history)]\n",
        "    #print(\"CONTEXT QUERY: \", (\"\\n\".join(question_history) + question))\n",
        "    response = query_engine.query((\"\\n\".join(question_history) + question))\n",
        "    for i, doc in enumerate(response.source_nodes, start=1):\n",
        "        #print(f\"Document {i}: {doc.node.text[:200]}...\")\n",
        "        #print(f\"Document Title: {doc.node.metadata.get('document_title', 'No title available')}\")\n",
        "        if i < 8: #taking top 8 results\n",
        "          cur_cont += f\"Document: {doc.node.metadata.get('document_title', 'No title available')} (Excerpt from text: {doc.node.text}) \\n\\n\"\n",
        "          titles.append(doc.node.metadata.get('document_title', 'No title available'))\n",
        "\n",
        "    # adding this new round of docs to the FRONT of the context string\n",
        "    context = cur_cont + context\n",
        "\n",
        "    return context, titles\n",
        "\n",
        "\"\"\"\n",
        "Explores prompt engineering to get a prompt that takes in context and a question.\n",
        "TODO: this doesn't really account for questions that CANNOT be answered in the dataset.\n",
        "\"\"\"\n",
        "def get_prompt(question):\n",
        "    context, titles = get_context(question)\n",
        "\n",
        "    formatted_conv = get_conv_hist()\n",
        "\n",
        "    # this prompt helps keep a consistent complete sentance format\n",
        "    # NOTE: the tags are specific to the instruct model, see https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/\n",
        "    prompt = f\"\"\"\n",
        "    <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "    You are a helpful AI assistant. You will analyze the information in the provides context passages and conversation history, and answer questions based solely on that context.\n",
        "    Answer the question based on the information in the passages.\n",
        "\n",
        "    - Do NOT reference the context chunks directly\n",
        "    - Respond in a complete sentence\n",
        "    - if the question cannot be answered based on the information in the passages, say so explicitly\n",
        "\n",
        "    Here is the history of your conversation with the user:\n",
        "    {formatted_conv}\n",
        "    <|eot_id|>\n",
        "\n",
        "    <|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "    Here is the relevant context:\n",
        "    {context}\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    <|start_header_id|>assistant<|end_header_id|>\n",
        "    \"\"\"\n",
        "\n",
        "    return prompt, titles\n",
        "\n",
        "\"\"\"\n",
        "Primary method called from the frontend.\n",
        "\n",
        "Takes in a question, formats the prompt and context, then passes the output into the model.\n",
        "\"\"\"\n",
        "def get_response(question):\n",
        "  global conv_history\n",
        "  FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "  prompt, titles = get_prompt(question)\n",
        "\n",
        "  #print(prompt)\n",
        "\n",
        "  inputs = tokenizer([prompt], return_tensors = \"pt\").to(\"cuda\")\n",
        "  #outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "  outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=150,\n",
        "        min_new_tokens=5, #key to avoiding empty inputs\n",
        "        temperature=0.1, # increase here = more likely to choose less probable tokens, increases creativity (we don't want that lol)\n",
        "        use_cache=True,\n",
        "        top_p=0.2, # similar to temp\n",
        "        #num_beams=3,\n",
        "        # turned this off to ensure consistency, cancells out temp and top_p\n",
        "        do_sample=False, # random samples groups of likely tokens, also introduces randomness that increases creativity\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        # Stop at the end of the answer\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        # Prevent prompt repetition\n",
        "        no_repeat_ngram_size=3\n",
        "    )\n",
        "\n",
        "  #response = tokenizer.batch_decode(outputs)\n",
        "  response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "  if not response: #this shouldn't ever happen bc we've set min_new_tokens but is here in case something goes wrong\n",
        "    return \"Hm, I can't seem to find an answer to that question in this dataset.\"\n",
        "\n",
        "  # adding both the question and response to the conversation history\n",
        "  set_conv_hist(question)\n",
        "  set_conv_hist(response.strip())\n",
        "\n",
        "  return response.strip(), titles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "os2GZ4e4FQoR"
      },
      "outputs": [],
      "source": [
        "# Testing inference\n",
        "\n",
        "# Example questions:  Why is it important to reduce gas emissions?\n",
        "# From what type of mine do most coal mine emissions come from?\n",
        "# When and where did the first occurance of methane drainage take place?\n",
        "# \"When and where was methane drainage first recorded?\"\n",
        "# \"Which documents should I reference to learn more about methane drainage?\"\n",
        "# \"From what type of coal mine does the most ventilation air methane come from?\" --> this one is good\n",
        "\n",
        "question = \"What is methane drainage?\"\n",
        "resp, titles = get_response(question)\n",
        "print(resp)\n",
        "print(\"\\n\")\n",
        "\n",
        "question = \"Can you explain in more detail?\"\n",
        "resp, titles = get_response(question)\n",
        "print(resp)\n",
        "print(\"\\n\")\n",
        "\n",
        "question = \"When and where did methane drainage first take place?\"\n",
        "resp, titles = get_response(question)\n",
        "print(resp)\n",
        "print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovnQa2h1ptbE"
      },
      "source": [
        "#Front end\n",
        "\n",
        "We run a flask app from the server in the \"Main Server\" subsection. This handles a basic frontend that displays an input for the user to ssend a question, then sends that question to our inference functions, which returns a response. We then display the response on the frontend and prompt the user again for input.\n",
        "\n",
        "INSTRUCTIONS to run the app:\n",
        "\n",
        "\n",
        "1.   Run all of the cells in the file until you reach the \"Main server\" section (shortcut: go to that cell, click on it, then go to Runtime->Run before to run every cell prior in the notebook)\n",
        "2.   Run the main server cell\n",
        "3.   After it starts, you'll see the following output (or similar):\n",
        "\n",
        "* Public URL: NgrokTunnel: \"https://8c7f-34-142-236-178.ngrok-free.app\" -> \"http://localhost:5000\"\n",
        " * Serving Flask app '__main__'\n",
        " * Debug mode: off\n",
        "INFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
        " * Running on http://127.0.0.1:5000\n",
        "\n",
        "4.   Click on the NgrokTunnel URL (NOT localhost or 127.0.0.1), then select OK when asked about security. This will be the page where you can see/interact with the bot.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo1i6nMxTsi0"
      },
      "source": [
        "## HTML Templates\n",
        "Run each cell to create the file, which will then be stored in the Colab file storage. This just prevents us from having to upload new files every time we run the Colab/change the HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CT4OjqizUS3-"
      },
      "outputs": [],
      "source": [
        "# First, create necessary directories and files\n",
        "!mkdir -p templates static/css\n",
        "!mkdir -p content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0D61iyFT4ji"
      },
      "outputs": [],
      "source": [
        "# Write HTML content to files\n",
        "%%writefile templates/base.html\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>{{ title }}</title>\n",
        "    <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='css/style.css') }}\">\n",
        "</head>\n",
        "<body>\n",
        "    <nav>\n",
        "        <ul>\n",
        "            <li><a href=\"{{ url_for('home') }}\">Home</a></li>\n",
        "        </ul>\n",
        "    </nav>\n",
        "\n",
        "    <main>\n",
        "        {% block content %}\n",
        "        {% endblock %}\n",
        "    </main>\n",
        "</body>\n",
        "</html>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfocrQvJUtVx"
      },
      "outputs": [],
      "source": [
        "%%writefile templates/home.html\n",
        "{% extends \"base.html\" %}\n",
        "\n",
        "{% block content %}\n",
        "<div class=\"container\">\n",
        "    <h1>Welcome to Flask</h1>\n",
        "    <p>This is your homepage with styled content!</p>\n",
        "</div>\n",
        "{% endblock %}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_S3VQwuo8X54"
      },
      "outputs": [],
      "source": [
        "%%writefile templates/index.html\n",
        "\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>GenAI-Bot</title>\n",
        "    <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='css/bot-style.css') }}\">\n",
        "    <script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js\"></script>\n",
        "</head>\n",
        "<body>\n",
        "    <header>\n",
        "        <h1>Questions about Climate Change?</h1>\n",
        "        <h4>Ask our interactive bot below to be directed to helpful resources</h4>\n",
        "    </header>\n",
        "\n",
        "    <main>\n",
        "        <div class=\"chat-container\">\n",
        "            <div id=\"chatbox\" class=\"chat-box\">\n",
        "                <p class=\"botText\">\n",
        "                    <span>Hello! How can I help you?</span>\n",
        "                </p>\n",
        "            </div>\n",
        "            <div id=\"userInput\" class=\"input-container\">\n",
        "                <input id=\"textInput\" type=\"text\" name=\"msg\" placeholder=\"Type your message...\" />\n",
        "                <!-- could add submit button here if wanted -->\n",
        "            </div>\n",
        "        </div>\n",
        "\n",
        "\n",
        "        <!-- what actually displays the relevant docs -->\n",
        "        <div id=\"relevantDocs\" class=\"relevant-docs\">\n",
        "            <h4>Relevant Documents:</h4>\n",
        "            {% for doc in rel_docs %}\n",
        "            <p>\n",
        "                <a href=\"{{ doc.url }}\" target=\"_blank\">{{ doc.title }}</a>\n",
        "            </p>\n",
        "            {% endfor %}\n",
        "        </div>\n",
        "\n",
        "        <div class=\"actions\">\n",
        "            <a href=\"{{ url_for('dataset')}}\" class=\"view-pdfs-button\">View all PDFs</a>\n",
        "        </div>\n",
        "\n",
        "        <br/>\n",
        "        <br/>\n",
        "        <br/>\n",
        "\n",
        "    </main>\n",
        "\n",
        "    <script>\n",
        "        function getBotResponse() {\n",
        "            var rawText = $(\"#textInput\").val();\n",
        "            var userHtml = '<p class=\"userText\"><span>' + rawText + \"</span></p>\";\n",
        "            $(\"#textInput\").val(\"\");\n",
        "            $(\"#chatbox\").append(userHtml);\n",
        "            document\n",
        "                .getElementById(\"userInput\")\n",
        "                .scrollIntoView({ block: \"start\", behavior: \"smooth\" });\n",
        "\n",
        "            $.get(\"/get\", { msg: rawText }).done(function(data) {\n",
        "                var botHtml = '<p class=\"botText\"><span>' + data.response + \"</span></p>\";\n",
        "                $(\"#chatbox\").append(botHtml);\n",
        "\n",
        "                var docsContainer = document.getElementById(\"relevantDocs\");\n",
        "                docsContainer.innerHTML = \"\"; // Clear existing docs\n",
        "                data.rel_docs.forEach(function(doc) {\n",
        "                    var p = document.createElement(\"p\");\n",
        "                    var a = document.createElement(\"a\");\n",
        "                    a.href = doc.url;\n",
        "                    a.textContent = doc.title;\n",
        "                    a.target = \"_blank\"; // Opens in new tab\n",
        "                    p.appendChild(a);\n",
        "                    docsContainer.appendChild(p);\n",
        "                });\n",
        "\n",
        "                document\n",
        "                    .getElementById(\"userInput\")\n",
        "                    .scrollIntoView({ block: \"start\", behavior: \"smooth\" });\n",
        "            });\n",
        "        }\n",
        "\n",
        "        $(\"#textInput\").keypress(function(e) {\n",
        "            if (e.which == 13) {\n",
        "                getBotResponse();\n",
        "            }\n",
        "        });\n",
        "    </script>\n",
        "</body>\n",
        "</html>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2-A8GATCFM5"
      },
      "outputs": [],
      "source": [
        "%%writefile templates/pdf_gallery.html\n",
        "\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>PDF Thumbnail Gallery</title>\n",
        "     <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='css/bot-style.css') }}\">\n",
        "    <link href=\"https://cdn.jsdelivr.net/npm/tailwindcss@2.2.16/dist/tailwind.min.css\" rel=\"stylesheet\">\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"back-button\">\n",
        "        <button> <a href=\"{{ url_for('home')}}\"> Back to Bot </a> </button>\n",
        "    </div>\n",
        "    <div class=\"container mx-auto my-8\">\n",
        "        <h1 class=\"text-3xl font-bold mb-4\">PDF Thumbnail Gallery</h1>\n",
        "        <div class=\"grid grid-cols-3 gap-4\">\n",
        "            {% for pdf in pdf_data %}\n",
        "            <div class=\"border rounded shadow p-4\">\n",
        "                <!-- <img src=\"{{ pdf.thumbnailUrl }}\" alt=\"{{ pdf.title }}\" class=\"w-full h-auto\"> -->\n",
        "                <a href=\"#\" onclick=\"openPdfInNewTab('{{ pdf.pdfUrl }}'); return false;\" class=\"block border rounded shadow p-4 hover:shadow-lg transition-shadow\">\n",
        "                  <img src=\"{{ url_for('static', filename=pdf.thumbnailUrl) }}\" class=\"image\" />\n",
        "                  <h3 class=\"mt-2 text-lg font-medium\">{{ pdf.title }}</h3>\n",
        "                </a>\n",
        "            </div>\n",
        "            {% endfor %}\n",
        "        </div>\n",
        "    </div>\n",
        "    <script>\n",
        "        function openPdfInNewTab(pdfUrl) {\n",
        "            window.open(pdfUrl, '_blank');\n",
        "        }\n",
        "    </script>\n",
        "</body>\n",
        "</html>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "978Iux3nVB0B"
      },
      "source": [
        "##CSS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ys_DY-Fm8iO-"
      },
      "outputs": [],
      "source": [
        "%%writefile static/css/bot-style.css\n",
        "\n",
        "\n",
        "* {\n",
        "    box-sizing: border-box;\n",
        "    margin: 0;\n",
        "    padding: 0;\n",
        "}\n",
        "\n",
        "body, html {\n",
        "    height: 100%;\n",
        "    font-family: 'Arial', sans-serif;\n",
        "    background-color: #f4f4f9;\n",
        "    color: #333;\n",
        "    line-height: 1.6;\n",
        "}\n",
        "\n",
        "header {\n",
        "    text-align: center;\n",
        "    padding: 20px;\n",
        "    background-color: #f4f4f9;\n",
        "    color: #4c87af;\n",
        "}\n",
        "\n",
        "header h1 {\n",
        "    margin-bottom: 10px;\n",
        "    font-size: 2rem;\n",
        "}\n",
        "\n",
        "header h4 {\n",
        "    font-weight: normal;\n",
        "}\n",
        "\n",
        ".chat-container {\n",
        "    max-width: 600px;\n",
        "    margin: 20px auto;\n",
        "    padding: 20px;\n",
        "    background: white;\n",
        "    border-radius: 10px;\n",
        "    box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1);\n",
        "}\n",
        "\n",
        ".chat-box {\n",
        "    max-height: 400px;\n",
        "    overflow-y: auto;\n",
        "    margin-bottom: 20px;\n",
        "    padding: 10px;\n",
        "    border: 1px solid #ddd;\n",
        "    border-radius: 5px;\n",
        "    background-color: #f9f9f9;\n",
        "}\n",
        "\n",
        ".input-container {\n",
        "    display: flex;\n",
        "    justify-content: space-between;\n",
        "}\n",
        "\n",
        "#textInput {\n",
        "    width: 100%;\n",
        "    padding: 10px;\n",
        "    border: 1px solid #ccc;\n",
        "    border-radius: 5px;\n",
        "    font-size: 16px;\n",
        "    outline: none;\n",
        "    transition: border-color 0.2s;\n",
        "}\n",
        "\n",
        "#textInput:focus {\n",
        "    border-color: #4c87af; //4CAF50;\n",
        "}\n",
        "\n",
        ".userText, .botText {\n",
        "    margin: 10px 0;\n",
        "    font-size: 16px;\n",
        "}\n",
        "\n",
        ".userText span {\n",
        "    background-color: #444;\n",
        "    color: white;\n",
        "    padding: 10px;\n",
        "    border-radius: 10px;\n",
        "    display: inline-block;\n",
        "}\n",
        "\n",
        ".botText span {\n",
        "    background-color: #4c87af;\n",
        "    color: white;\n",
        "    padding: 10px;\n",
        "    border-radius: 10px;\n",
        "    display: inline-block;\n",
        "}\n",
        "\n",
        ".actions {\n",
        "    text-align: center;\n",
        "    margin: 20px 0;\n",
        "}\n",
        "\n",
        ".view-pdfs-button {\n",
        "    display: inline-block;\n",
        "    padding: 10px 20px;\n",
        "    background-color: #4c87af;\n",
        "    color: white;\n",
        "    text-decoration: none;\n",
        "    border-radius: 5px;\n",
        "    transition: background-color 0.3s;\n",
        "}\n",
        "\n",
        ".view-pdfs-button:hover {\n",
        "    background-color: #456ba0; //45a049;\n",
        "}\n",
        "\n",
        ".relevant-docs {\n",
        "    max-width: 600px;\n",
        "    margin: 20px auto;\n",
        "    padding: 10px;\n",
        "    background: #f9f9f9;\n",
        "    border-radius: 5px;\n",
        "    box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1);\n",
        "}\n",
        "\n",
        ".relevant-docs p a {\n",
        "    color: #333;\n",
        "    text-decoration: none;\n",
        "}\n",
        "\n",
        ".relevant-docs p a:hover {\n",
        "    text-decoration: underline;\n",
        "}\n",
        "\n",
        "footer {\n",
        "    text-align: center;\n",
        "    padding: 10px 0;\n",
        "    background: #4c87af;\n",
        "    color: white;\n",
        "    position: fixed;\n",
        "    bottom: 0;\n",
        "    width: 100%;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQqT5Wc1Tpv7"
      },
      "source": [
        "## Main server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgcbeXeaGLni"
      },
      "outputs": [],
      "source": [
        "# from https://colab.research.google.com/drive/10doc9xwhFDpDGNferehBzkQ6M0Un-tYq#scrollTo=QDVm2QUrnJaF\n",
        "!apt-get install poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMgpnTTfss1Q"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, request, render_template, jsonify\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "import shutil\n",
        "from pdf2image import convert_from_path\n",
        "from googleapiclient.discovery import build\n",
        "from google.oauth2 import service_account\n",
        "import json\n",
        "\n",
        "\"\"\" uses GDrive API to find folder with a given name \"\"\"\n",
        "def find_folder(drive_service, folder_name):\n",
        "    # getting all folders shared with the drive API\n",
        "    all_folders = drive_service.files().list(\n",
        "        q=\"mimeType='application/vnd.google-apps.folder' and trashed=false\",\n",
        "        fields='files(id, name)',\n",
        "        spaces='drive'\n",
        "    ).execute()\n",
        "\n",
        "    folders = all_folders.get('files', [])\n",
        "    for folder in folders:\n",
        "        if folder['name'].lower() == folder_name.lower():\n",
        "            return folder['id']\n",
        "\n",
        "    return None\n",
        "\n",
        "\"\"\" gets a folder with GDrive API, then pulls all PDFS from the folder \"\"\"\n",
        "def get_pdfs():\n",
        "    colab_dir = '/content/static'\n",
        "    os.makedirs(colab_dir, exist_ok=True)\n",
        "\n",
        "    # initialize Google Drive API\n",
        "    creds = service_account.Credentials.from_service_account_info(\n",
        "        info=json.load(open('/content/service_account.json', 'r'))\n",
        "    )\n",
        "    drive_service = build('drive', 'v3', credentials=creds)\n",
        "\n",
        "    folder_id = find_folder(drive_service, 'UNICC_dataset')\n",
        "\n",
        "    if not folder_id:\n",
        "        raise Exception(\"Could not find the UNICC_db folder. Please check folder sharing permissions.\")\n",
        "\n",
        "    # Search for PDF files in the folder\n",
        "    file_list = drive_service.files().list(\n",
        "        q=f\"'{folder_id}' in parents and mimeType='application/pdf' and trashed=false\",\n",
        "        fields='files(id, name, webViewLink)',\n",
        "        pageSize=100,\n",
        "        spaces='drive'\n",
        "    ).execute()\n",
        "\n",
        "    pdf_files = file_list.get('files', [])\n",
        "    print(f\"\\nFound {len(pdf_files)} PDF files:\")\n",
        "    for pdf in pdf_files:\n",
        "        print(f\"- {pdf['name']} (ID: {pdf['id']})\")\n",
        "\n",
        "    if not pdf_files:\n",
        "        # If no PDFs found, check what files are actually in the folder\n",
        "        all_files = drive_service.files().list(\n",
        "            q=f\"'{folder_id}' in parents and trashed=false\",\n",
        "            fields='files(id, name, mimeType)',\n",
        "            pageSize=100\n",
        "        ).execute()\n",
        "        print(\"\\nAll files in folder:\")\n",
        "        for file in all_files.get('files', []):\n",
        "            print(f\"- {file['name']} ({file['mimeType']})\")\n",
        "\n",
        "    # extracting text from found PDFs\n",
        "    pdf_data = []\n",
        "    for file in pdf_files:\n",
        "        filename = file['name']\n",
        "        print(f\"\\nProcessing {filename}...\")\n",
        "        local_path = os.path.join(colab_dir, filename)\n",
        "\n",
        "        try:\n",
        "            request = drive_service.files().get_media(fileId=file['id'])\n",
        "            with open(local_path, 'wb') as f:\n",
        "                f.write(request.execute())\n",
        "            print(f\"Downloaded {filename}\")\n",
        "\n",
        "            # generating thumbnail image\n",
        "            first_page = convert_from_path(local_path, last_page=1)[0]\n",
        "            thumbnail_path = os.path.join(colab_dir, f\"{os.path.splitext(filename)[0]}.png\")\n",
        "            first_page.save(thumbnail_path, 'PNG')\n",
        "            print(f\"Generated thumbnail for {filename}\")\n",
        "\n",
        "            pdf_data.append({\n",
        "                'pdfUrl': file['webViewLink'],\n",
        "                'thumbnailUrl': f\"{os.path.splitext(filename)[0]}.png\",\n",
        "                'title': os.path.splitext(filename)[0]\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file {filename}: {str(e)}\")\n",
        "            continue\n",
        "        finally:\n",
        "            if os.path.exists(local_path):\n",
        "                os.remove(local_path)\n",
        "                print(f\"Cleaned up {filename}\")\n",
        "\n",
        "    return pdf_data\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return render_template('index.html', title='Home')\n",
        "\n",
        "@app.route(\"/get\")\n",
        "def get_bot_response():\n",
        "    userText = request.args.get('msg')\n",
        "    response, titles = get_response(userText)\n",
        "    print(\"updated titles: \",titles)\n",
        "\n",
        "    # getting URLs for all of the titles\n",
        "    titles = set(titles) # removing duplicates\n",
        "    rel_docs = []\n",
        "    for title in titles:\n",
        "        for pdf in pdf_data:\n",
        "            if pdf['title'] == title:\n",
        "                rel_docs.append({\"title\": title, \"url\": pdf['pdfUrl']})\n",
        "                break\n",
        "\n",
        "    return jsonify({\n",
        "        'response': response,\n",
        "        'rel_docs': rel_docs\n",
        "    })\n",
        "\n",
        "@app.route(\"/dataset\")\n",
        "def dataset():\n",
        "\n",
        "    return render_template('pdf_gallery.html', pdf_data=pdf_data)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # loading PDFs once bc it takes forever\n",
        "    try:\n",
        "        pdf_data = get_pdfs()\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading PDF data: {str(e)}\")\n",
        "        pdf_data = []\n",
        "\n",
        "    # Get a tunnel from ngrok and run Flask\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print(f' * Public URL: {public_url}')\n",
        "\n",
        "    # Run the app\n",
        "    app.run(port=5000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
